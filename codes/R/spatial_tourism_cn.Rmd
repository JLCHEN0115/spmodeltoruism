---
title: "Tourism demand and revenue with sptial effects"
author: "Jialiang"
output:
  html_document: 
    toc: true
    theme: united
  pdf_document: default
date: "2023-03-20"
---

## Requirements

### External libraries

For Linux and macOS (I'm in M2) users who are working with spatial data in R, you may encounter error messages like this during package downloading (if you run `install.packages("sf")`, for example).
```
*configure: error: libproj or sqlite3 not found in standard or given locations.*
```
This is because they need additional (non-R) libraries such as PROJ, GDAL or GEOS (which are automatically installed for Windows users). It is highly recommend to install the CRAN binary packages by running in R terminal
```{r eval=FALSE}
install.packages("sf", type = "mac.binary")
```

If you do want to download the source package, you should refer to [https://r-spatial.github.io/sf/#macos](https://r-spatial.github.io/sf/#macos) and and use the similiar specification like `configure.args =` for additional configuration.

### R packages
Load and install the R packages we might will be using.(This is an inclusive list, and I did this purposely for future reference. I commented those not useful in the codes.)  
```{r, cache=FALSE, message=FALSE}
## Download(if needed) and load package management tool.
if (!require("pacman")) install.packages("pacman")
## Download(if needed) and load other package for spatial analysis.
## pacman::p_load(sf, tidyverse, data.table, hrbrthemes, lwgeom, rnaturalearth, maps, mapdata, spData, tigris, tidycensus, leaflet, mapview, tmap, tmaptools, zoo, pspatreg, spatialreg, spdep, sf, plm, splm, pspatreg)
## Download(if needed) and load other package for use in this document.
pacman::p_load(sf, tidyverse, data.table, hrbrthemes, pspatreg, spatialreg, spdep, sf, plm, splm, pspatreg, zoo)
## ggplot2 plotting theme
theme_set(hrbrthemes::theme_ipsum())
```

```{r}
## Download and load our companion package.
## devtools::install_github("JLCHEN0115/spatial2023")
## library("spatial2023")
```
## Data wrangling
### Reading in the spatial data. 
Here I will use the absolute path. Replace the path of the `cities_included.shp` file on your computer. The data will be included in the companion package later on.
```{r, message = FALSE}
# Replace the absolute path of the `cities_included.shp` file on your computer. 
# Drag the file to terminal (command + space, then search `terminal` on spotlight) if you are in mac, the path will appear.
ct_shape <- st_read("/Users/jialiangchen/Documents/spmodeltoruism/shapefiles/china_second_level_admin_shape/cities_included.shp")
```

Import more data.
```{r, message = FALSE}
## change the absolute path on your computer, same as above
ct_data <- read.csv("/Users/jialiangchen/Documents/spmodeltoruism/data/dataforR.csv")
```

Perform a left join for our datasets.
```{r, message = FALSE}
ct_spdata_wide <- left_join(
  ct_shape %>% select(NL_NAME_2, geometry) %>% rename(城市shapefile = NL_NAME_2),  
  ct_data,
  by = "城市shapefile"
)

```
Reshape our data to long(tidy) form.
```{r}
ct_spdata_long <- ct_spdata_wide %>% 
  pivot_longer(
    cols = tarl_2011:grnld_2019,
    names_to = c(".value", "year"),
    names_pattern = "(.+)_(.+)"
  )
ct_spdata_long
```

As you can see above. Our dataset contains 283 prefectual level cities in 9 year, which give us a total number of $283 \times 9 = 2547$ observations. Type `str(ct_spdata_long) ` and `View(ct_spdata_long)` for more information.

We can draw plots using `ggplot2`. (For mysterious reasons, R draw graphs freakingly slow here on my computer. It spend me 15 mins + to get a plot in base R from terminal. A bug to be addressed in the future). I skiped the evaluation of the next chunk of code and just included the graph here.
```{r eval=FALSE}
plotfilter2015 <- ct_spdata_long %>% filter(year == 2015) %>% ggplot() +
  geom_sf(aes(fill = tarl), alpha = 0.8, col = "white")+
  scale_fill_viridis_c(name = "Arrivals") +
  ggtitle("Cities included in the research.")
plotfilter2015
```
![](/Users/jialiangchen/Documents/spmodeltoruism/figures/figure1.png){width=50%}

## Spatial Autocorrelation check
Let's check some Moran's I statistics. Before that, we need to prepare a list of weights based on neighboring relationships.`spdep::poly2nb` produces such neighboring relationships, and then `spdep::nb2listw` turns them into weights.

```{r}
## Use cross-sectional data from 2015 for the moment.
filter2015 <- ct_spdata_long %>% filter(year == 2015)
## Create a weighting matrix based on contiguity relations.
filter2015 %>% spdep::poly2nb("geometry") %>% 
  spdep::nb2listw(zero.policy = TRUE) -> Wnb
```

```{r}
## Identify coordinates of the centroid of the multipolygon
coords <- st_centroid(st_geometry(ct_spdata_wide))

## Create our 3,4,5,6 NN matrix
## Note that the latitude and longitude are handled using great circle distances
## R2 distances will be inaccurate
knn3W <- knearneigh(coords, k = 3)
knn4W <- knearneigh(coords, k = 4)
knn5W <- knearneigh(coords, k = 5)
knn6W <- knearneigh(coords, k = 6)

## Convert our knn objects to neighborhood list
list3nn <- knn2nb(knn3W)
list4nn <- knn2nb(knn4W)
list5nn <- knn2nb(knn5W)
list6nn <- knn2nb(knn6W)

## Convert to a matrix object
matrix3nn <- spdep::nb2mat(list3nn)
matrix4nn <- spdep::nb2mat(list4nn)
matrix5nn <- spdep::nb2mat(list5nn)
matrix6nn <- spdep::nb2mat(list6nn)

## Convert our neighborhood list to an listw object
listw3nn <- spdep::nb2listw(list3nn) 
## Plot the neighborhood relationships
# plot(list4nn, coords)
```


### Global Moran's I statistics
```{r}
## Calculate the Moran's I test with matrix Wnb and year 2015.
spdep::moran.test(filter2015$tarl, listw3nn, zero.policy = TRUE)
```
The result shows a positive Moran I statistics, as we expected. It is both substantively ($\approx 0.16$) and statistically significant (with a p-value $0.00001139$).

We could try different weighting schemes and different years, but let's draw a Moran scatter-plot. The solid line in the plot indicates in the Moran’s I statistic. It goes though the mean values and its slope is exactly the Moran’s I.
```{r}
 spdep::moran.plot(filter2015$tarvl, 
                   Wnb, 
                   zero.policy = TRUE, 
                   xlab = 'Total Tourism Arrivals',
                   ylab = 'Lagged Total Tourism Arrivals (of Neighbors)',
                   pch=20)
```

### Local Moran's I statistics
We can use Local Moran’s I or Local Indicators of Spatial Association (LISA). LISA can help identify clusters of high or low values as well as outliers that are surrounded by opposite values.

```{r}
# use spdep package to test (global autocorrelation)
# spdep::localmoran, spdep::localmoran.exact
# local moran result: 
##  High positive Ii means simlar values (either high or low clusters)
##  Low negative Ii means dissimilar values (outliers)
##  

lisaRslt <- spdep::localmoran(filter2015$tarvl, Wnb, 
                              zero.policy = TRUE, na.action = na.omit)

## The dimension of LISA result and the orignal sf object
dim(lisaRslt)
dim(filter2015)
## Show some of the results.
head(lisaRslt)
```
Note that `Ii` stands for the local moran statistic, `E.Ii` is the expectation of local moran statistic, `Var.Ii` is the variance, `Z.Ii` is the standard deviation, and `Pr(z > 0)` is the p-values.

Assuming statistical significance, if $\text{Ii} > 0$,  it would be a cluster (similar to nearby or neighboring values, HH or LL); if $\text{Ii} < 0$, it would be a outlier (very different from nearby or neighboring values, HL or LH).

```{r}
# Now we can derive the cluster/outlier types for each spatial feature in the data
significanceLevel <- 0.05 # 95% confidence
meanVal <- mean(filter2015$tarvl)

lisaRslt %<>% tibble::as_tibble() %>%
  magrittr::set_colnames(c("Ii","E.Ii","Var.Ii","Z.Ii","Pr(z > 0)")) %>%
  dplyr::mutate(coType = dplyr::case_when(
  `Pr(z > 0)` > 0.05 ~ "Insignificant",
  `Pr(z > 0)` <= 0.05 & Ii >= 0 & filter2015$tarvl >= meanVal ~ "HH",
  `Pr(z > 0)` <= 0.05 & Ii >= 0 & filter2015$tarvl < meanVal ~ "LL",
  `Pr(z > 0)` <= 0.05 & Ii < 0 & filter2015$tarvl >= meanVal ~ "HL",
  `Pr(z > 0)` <= 0.05 & Ii < 0 & filter2015$tarvl < meanVal ~ "LH"
))

# Now add this coType to original sf data
filter2015$coType <- lisaRslt$coType %>% tidyr::replace_na("Insignificant")

ggplot(filter2015) +
  geom_sf(aes(fill=coType),color = 'lightgrey') +
  scale_fill_manual(values = c('red','brown','green','blue','cyan'), name='Clusters & \nOutliers') +
  labs(title = "Tourism arrivals of our included cities, year2015")
```


## Spatial Corss-sectional regressions
### Spatial Error Model
As a point of departure, we do an OLS and test whether spatial autocorrelation is present in the residuals.
```{r}
olsRslt <- lm(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
              data = filter2015,)
summary(olsRslt)
```
```{r}
# Derive the residuals from the regression. Need to handle those missed values.
lmResiduals <- rep(0, length(filter2015$tarvl)) #create a bunch of zeros
resIndex <- olsRslt$residuals %>% names() %>% as.integer() #create index for our 283 residuals
lmResiduals[resIndex] <- olsRslt$residuals

# Test if there is spatial autocorrelation in the regression residuals (errors).
Wnb %>%
  spdep::moran.test(lmResiduals, ., zero.policy = TRUE)
  
```
Our result show statistically and economically significant Moran I statistic, which implies that the residuals from linear regression have positive spatial autocorrelation. With that, we do a SAR error model.

$$Y = X\beta + u, \qquad \text{where} \quad u = \lambda Wu + \epsilon.$$
```{r}
# use spdep package to run the spatial error model 
# Use spatialreg::errorsarlm to run the same model
serrRslt <- spatialreg::errorsarlm(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
              data = filter2015,
              listw = Wnb,
              # Durbin = TRUE, #uncomment for including lagged X
              zero.policy = TRUE, 
              na.action = na.omit);

summary(serrRslt)
```
From the AIC, the spatial error model performs better than the linear model. Now we test if there is spatial dependence left after we controled that for the error term.
```{r}
# Derive the residuals from the regression. Need to handle those missed values.
seResiduals <- rep(0, length(filter2015$tarvl))
resIndex <- serrRslt$residuals %>% names() %>% as.integer();
seResiduals[resIndex] <- serrRslt$residuals

# Test if there is spatial autocorrelation in the regression residuals (errors).
Wnb %>%
  spdep::moran.test(seResiduals, ., zero.policy = TRUE) 
```
We cannot reject the null. We are good to go.

### Spatial Lag Model
This time, we estimate the following model, aka. The spatial lag model.
$$Y = \rho WY + X\beta + \epsilon.$$
```{r}
# use spdep package to run the spatial lag model 
# spatialreg::lagsarlm 
slmRslt <- spatialreg::lagsarlm(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
              data = filter2015,
              listw = Wnb,
              # Durbin = TRUE, #uncomment for including lagged X
              zero.policy = TRUE, 
              na.action = na.omit);

summary(slmRslt)
```

We can also calculate the direct, indirect and total impacts.
```{r}
trMnb <- trW(listw = Wnb, type= "moments")
impacts(slmRslt, tr = trMnb)
```

The spatial lag term $\rho$ is highly statistically and economically significant (greater than $0$).

```{r}
# Derive the residuals from the regression. Need to handle those missed values
slResiduals <- rep(0, length(filter2015$tarvl))
resIndex <- slmRslt$residuals %>% names() %>% as.integer();
slResiduals[resIndex] <- slmRslt$residuals

# Test if there is spatial autocorrelation in the regression residuals (errors) after we corrected the spatial dependence by including the spatially lagged dependent variable.
Wnb %>%
  spdep::moran.test(slResiduals, ., zero.policy = TRUE)
```

### SARAR model
There is..., but let's run a "SAC/SARAR" model.
$$y = \rho Wy + X\beta + u, \qquad\text{where}\quad u = \lambda Wu + \epsilon.$$
```{r}
# use spdep package to run the spatial lag and spatial error model 
# spatialreg::sacsarlm
sararRslt <- spatialreg::sacsarlm(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
              data = filter2015,
              listw = Wnb,
              # Durbin = TRUE, #uncomment for including lagged X
              zero.policy = TRUE,
              na.action = na.omit)

summary(sararRslt)
```

## Spatial panel regressions

### Preparation
My feeling is that panel data are sensitive to NAs and empty neighborhoods in R. With that, I will first construct a new weighting matrix W, and then deal with the NAs in our data.

Take a look of our where do our NAs inhabit.
```{r}
which(ct_spdata_long %>% is.na()  %>% colSums() > 0) %>% names()
```

Take a look of our old weighting scheme (Recall that it was based on continguity).
```{r}
summary(Wnb, zero.policy=TRUE)
```

As you can see, there are 7 regions with no links: 79(海口) 80(三亚) 202(西宁) 260(乌鲁木齐) 261(克拉玛依) 262(拉萨) 281(舟山). We should deal with these two problems in turn.

#### 4 Nearest Neighbors Weighting matrix
To avoid islands, we contruct a new meighting matrix by 4 NN.
```{r, results="hide"}
## Identify coordinates
coords <- cbind(ct_data$Long, ct_data$Lat)

## Create our 4 NN matrix
## Note that the latitude and longitude are handled using great circle distances
## R2 distances will be inaccurate
matrix5nn <- knearneigh(coords, k = 4, longlat = TRUE)

## Convert our weighting matrix to neighborhood list
W5nnb <- knn2nb(matrix5nn)
## Plot the neighborhood relationships
plot(W5nnb, coords)
## Convert our neighborhood list to an listw object
W5nnlistw <- spdep::nb2listw(W5nnb, zero.policy = FALSE) 
```
#### Interpolate the missing values
We do our interpolation by simple(crude?) linear interpolation. This actually make sense since in our `ct_spdata_long` dataset, most of the cities are arranged based on their provinces, also we have nine time periods for each city. Linear interpolationThey should be fine because of the geographical similarity and time trends.
```{r}
## We only interpolate the variables we will be using in this document
## library(zoo)
ct_spdata_long <- ct_spdata_long %>%
        mutate(CPIp = na.approx(CPIp), salary = na.approx(salary), area = na.approx(area), pop = na.approx(pop), railway = na.approx(railway))
```

### Spatial Lag panel model
Since we changed the weighting scheme and did an interpolation, the following result is not directly comparable with the corss-sectional results above.

```{r}
## random effects panel with spatial lag
sar_panel <- spml(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
               data = ct_spdata_long,
               index=c("城市", "year"),
               listw = W5nnlistw,
               model="random", 
               spatial.error="none", 
               lag=TRUE)
summary(sar_panel)
```
```{r}
## impacts
impac_panel <- spatialreg::impacts(sar_panel, listw = W5nnlistw, time = 9)
summary(impac_panel, zstats=TRUE, short=TRUE)
```
### Spatial Error panel model
```{r}
## fixed effects panel with spatial errors
serr_panel <- spml(log(tarvl+1) ~ CPIp +
                log(1+salary) +
                log(1+area) + log(1+pop) +
                log(1+railway),
               data = ct_spdata_long,
               index=c("城市", "year"),
               listw = W5nnlistw,
               model="within", 
               spatial.error="b", 
               Hess = FALSE)
summary(serr_panel)
```

## Summary
I first merged the shape (geometry) file with our other data. The Spatial Autocorrelation check by Moran's I is fine. Spatial Corss-sectional regressions using year 2015 points us to an error model, which might be problematic. It could be that we have too much left in the error term. Then we run the Spatial panel regressions, the results are quite good.

Things to notice:

* Note that the `CPIp` is not significant both statistically and economically. Need to address this issue. (the minimum of `CPIp` is $99.74$, and the third quantile is $102.45$ - there isn't many variation in the variable.)

* All estimation in the document is done by maximum likelihood methods. Importantly, this assumes the endogeneity of the right-handside variables other than the spatiall lagged $y$ and the normality, homoskedasticity or the error term. Be cautious here.

* Spatial heterogeneity will be addressed in another document.
