\documentclass[11pt,a4paper]{amsart}
\usepackage{setspace}
\doublespacing
\usepackage{amssymb,latexsym}
\usepackage{graphicx}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{axiom}{Axiom}
\newtheorem{proposition}{Proposition}
\usepackage{geometry}
\geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1cm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\usepackage{ulem} % various underlines
\usepackage{hyperref} % to insert URL 
\usepackage{graphicx} % to insert illustration
\usepackage[mathscr]{eucal} % to express a collection of sets
\usepackage{bm} % bold font in equation environment
\usepackage{color} % color some text
\usepackage{framed} % to add a frame 
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=1pt] (char) {#1};}} % circled numbers
\usepackage{float}%do not auto repositioning
\usepackage[style=apa, eprint=false]{biblatex} %Imports biblatex package
\raggedright
\addbibresource{specms.bib} %Import the bibliography file

% $\uppercase\expandafter{\romannumeral1}$ Roman numeral
%	\begin{figure}[hbt]
	%{\centering \includegraphics[scale=0.78]{ring_algebra_semi}}
	%\caption{ring \& algebra \& semi-}\label{F:ring_algebra_semi}
	%\end{figure}
% \usepackage[style=apa, eprint=false]{biblatex} %Imports biblatex package
% \addbibresource{name_of_bib.bib} %Import the bibliography file

\begin{document}
\title{Math Notes}
\author{Jialiang Chen} 
\date{\today}
\maketitle
\tableofcontents

\section{Spatial Stochastic Process}
There are three approaches to embed a spatial structure in our stochastic process analysis. They are direct representations, non-parametric functions and spatial process models. The spatial process models could be simultaneous models and conditional models. They are very distinct models. 

The simultaneous models is a model for the complete pattern, so we have the endogeneous variables $y$ on the left-hand side, and the exogenous variables $X$ on the right-hand side. We want to explain the variable $y$ at all locations being explained simultaneously. The conditional models are mainly for prediction or priors for parameters in Bayesian hierarchical models. It is very common in literature that people explaining simultaneous models using the conditional language, which is not appropriate.

\subsection{Simultaneous Spatial Process Models}\hfill\par 
The variance-covariance matrix comes from the spatial random process you specify. For example, in the (Simultaneous) Spatial Autoregressive Process, we have the reduced form expression
\[	y = (I - \rho W)^{-1}u.	\]
It follows that (assuming $\mathbb{E}[y] = 0$)
\[	\begin{aligned}
	\operatorname{Var}(y) &= \mathbb{E}[yy']\\
	&= \mathbb{E}[(I - \rho W)^{-1}uu'(I-\rho W')^{-1}] \\
	&= (I - \rho W)^{-1}\mathbb{E}[uu'](I-\rho W')^{-1}.
\end{aligned}	\]
Therefore, 
\[		\operatorname{Var}(y)  = \sigma^{2} [ (I - \rho W)'(I - \rho W)]^{-1}	\]
under the assumption of homoskedasticty $\operatorname{Var}(u_{i}) = \sigma$ and independence.

\subsection{Conditional Spatial Process Models}\hfill\par 
The general idea is that we want to get the joint distribution $(y_{1}, y_{2}, \dots, y_{n})$ from the conditional distributions $y_{i} \mid y_{-i}$ by a process called \textit{factorization}, which is a product of the conditional and marginal distributions. 

This is not hard in time series. We have the Markov Property in time to simplify the factorization. In space, however, building a joint distribution from conditionals  does not necessarily work. We we specify distribution conditional on neighbors, and call these the \textit{Markov Random Field (MRF)}. The Hammersley-Clifford Theorem builds the connection between MRF and the joint distribution to make sure that our Markov Random Field defines a unique joint distribution, under certain (pretty restrictive) conditions. The reverse of this process is called the \textbf{Gibbs Sampling}. It should be noted that most of the time, the resulting joint distribution is \textit{not} a multivariate version of the conditionals, unless we are in the Gaussian world. 

\section{Specification of Spatial Dependence}
The spatial lag model and the spatial error models are the most common ways to specify the spatial dependence. There are also some other specifications. For example, the spatial Durbin model and the SLX models. However, these models should be taken with a grain of salts. 

$\bullet$ Be aware of the inverse problem. Different processes can yield the same pattern. For example, the heteroskedasticity in the variance-covariance matrix of $y$ could be the result of the `true' heteroskedasticity  in the error term $u$, or could be the result of uneven number of neighbors in the weighting matrix. It is usually hard to look back and infer what is the origin of the pattern. 

$\bullet$ There are some motivations for specifying the spatial dependence structure. However, there are also a lot of ad-hoc maneuvers. Need more theory to address these problems. The new economic geography critique by \textcite{gibbonsMostlyPointlessSpatial2012}. It is difficult to interpret causal effects.

\subsection{The Mixed Regressive-Spatial Autoregressive Model}\hfill\par 
The model is specified as 
\[	y = \rho Wy + X \beta + u, 	\]
where $Wy$ is the spatial autoregressive (spatial lag) term, $X$ is the regressive term and $\rho$ is the spatial autoregressive coefficient. We could write it as 
\[	(I-\rho W) y = X \beta + u.	\]

You could think $(I-\rho W)$ as a spatial filter, which adjusts for spatial correlation. This provides another, more practical reason for including the spatial structure than the behavior motivations. If $y_{i}$ are highly spatially correlated, there would not be enough variations for statistical analysis. We like variations, and $(I-\rho W)$ is a spatial filter that adjusts for it. (We still need to estimate $\rho$. )

The \textbf{Spatial Multiplier} is derived from the reduced form. We have 
\[	\begin{aligned}
	\mathbb{E}[y \mid \triangle X] &= (I-\rho W)^{-1}(\triangle X)\beta \\
	&= [I + \rho W + \rho^{2}W^{2} + \dots ] (\triangle X) \beta. 
\end{aligned}	\]

It is intuitive to see the multiplier effect by writing $(I-\rho W)^{-1}$ as a series. 

The total effect of a change in $X$ is $ (I-\rho W)^{-1}(\triangle X)\beta$, the direct effect is defined as $(\delta X) \beta$, and the indirect effect is defined as $[(I-\rho W)^{-1} - I](\triangle X) \beta$, or $[\rho W + \rho^{2}W^{2} + \dots ] (\triangle X) \beta$.

We could apply the spatial multiplier in policy analysis and to simulate the spatial imprint of a policy change by sollving the reduced form for a change in $X$. Read Valuing Access To Water - A Spatial Hedonic Approach Applied To Indian Cities by \textcite{anselinValuingAccessWater2008}.

The misspecification of the spatial lag model is basically a disaster. It is a omitted variable problem, which leads to biased and inconsistent results, and also larger standard deviations (so don't say I am not interested in inference, it also matters if you only want to see the overall result). 

\subsection{Spatial Error Model}
This model is not very interesting, as there is no substantive interpretation. An error is just an error. If there are spatial patterns in the error term, we extend our specification as far as we can, either by including new exogeneous variables, or by imposing structures on the error terms. 

The model is 
\[	y = X\beta + u	\]
with $u = \lambda Wu + e$.

The reduced form is 
\[	y = X\beta + (I-\lambda W)^{-1}e.	\]

Notice that there is simply no substantive multiplier effect. The only spatial effect is on the error term, which disappears on average. This model could be used for \textit{kriging} (spatial prediction).

The misspecification (consequence of ignoring SAR errors) is not that severe. The OLS remains unbiased, but it is not efficient. 

\subsection{Spatial Durbin Model}\hfill\par 
The spatial Durbin model has been widely used, but many of them are for wrong reasons (or, no reason). 
\subsubsection{The Classic Spatial Durbin Model}\hfill\par 
The point here is, the Classic Spatial Durbin Model is equivalent to the Spatial Error model. We start with rewriting a SAR model. 
\[	y  = X\beta + u 	\]
with $u = \lambda W u + e$. 
By substitution, we have 
\[	y = X\beta + (I-\lambda W)^{-1}e.	\]
Pre-multiply $(I-\lambda W)$ on both sides of the equation gives us
\[	(I-\lambda W)y = (I-\lambda W)X\beta + e.	\]

This is a spatially filtered variable regression (spatially filtered $y$ and $x$).

Now, we rewrite this equation as 
\begin{equation}\label{sdm}
	y = \lambda W y + X \beta - \lambda WX \beta + u.
\end{equation}

This is called the Classic Spatial Durbin Model. It is a non-linear model in $\lambda$ and $\beta$. Note that the coefficient on $WX$ is just the negative of the multiplication of the coefficients on $Wy$ and $X$. This gives us $k-1$ tests naturally, which is called the \textit{Common Factor Hypothesis}, and could be used to test if the SDM is an appropriate specification. (More on this later.)

Note that if we cannot reject $H_{0}: \lambda = 0$, then we are back to the standard regression model.

Note that the constant term is not \textit{separately identifiable}. The constant terms in $X$ is a column of $1$s. Note that the spatially lagged column of $1$s is not changed. The resulting constant term in this model is $(1-\lambda) \beta_{0}$. We can only identify this product, but not separately $\lambda$ and $\beta_{0}$. 

\subsubsection{The Unconstrained Spatial Durbin Model}\hfill\par 
Somehow, people started estimating
\[	y = \gamma_{1}Wy + X\gamma_{2} + WX\gamma_{3} + u,	\]
which is totally fine. But it is important to realize that the \textit{common factor hypothesis} is still there. We can test for the null hypothesis (scalar multiplication here, $\gamma_{1} \in \mathbb{R}$ and $\gamma_{2}, \gamma_{3} \in \mathbb{R}^{k}$.)
\[	\operatorname{H_{0}}:\quad \gamma_{1} \gamma_{2} = -\gamma_{3}.	\]
We are in an awkward place here. If $H_{0}$ is \textit{not} rejected, then we are in equation (\ref{sdm}), which is nothing but the SAR error model. \emph{And recall that, there is no multiplier effect in the error model.} If the $H_{0}$ is rejected, the problem is we do not know where to go - we have different interpretations. We know that this is not an SAR error process, but if we also reject $H_{0}: \gamma_{3} = 0$, this does not imply the spatial lag model. This means that these models are not nested here. 

The reduced form is 
\[	y = (I-\gamma_{1}W)^{-1}X\gamma_{2} + (I-\gamma_{1}W)^{-1}WX\gamma_{3} + v,	\]
where $x = (I-\gamma_{1}W)^{-1}u$.

Now, to look for the direct and indirect effect, we expand the inverse term out, and get
\[	y = (I + \gamma_{1}W + \gamma_{1}^{2}W^{2} + \dots )X\gamma_{2} + (I+\gamma_{1}W + \gamma_{1}^{2}W^{2} + \dots )WX\gamma_{3} + v.	\]

This is by no means simple. Note that a change in $X$ has multiple spatial effects. It is argued that the spatially lagged model already has $WX$ in it, so by adding $WX$ in addition to $Wy$, we have it twice. There should be a reason why it should be there twice. This makes the interpretation of the direct and indirect effects very complicated. 

\section{Specification of Spatial Heterogeneity}\hfill\par 
\subsection{Spatial Regimes}\hfill\par 
\subsubsection{Heterogeneity in the intercepts}
$\bullet$ Varying intercepts. The standard $t-$tests are not correct in the presence of spatial autocorrelation. In order to fix that problem, we could turn the tests (e.g., the ANOVA tests) into regression forms by adding indicator variables for each regimes, which is equivalent to the original tests, and then incorporate spatially lagged variables if necessary (as suggested by specification tests). 

$\bullet$ Spatial fixed effects. Usually in muli-level specification, for example, the units in census tracts. We include a reference mean $\alpha$ and the differences by regimes in the regression as follows
\[	y_{ij} = \alpha + \alpha_{2}d_{i2} + \dots + \alpha_{G}d_{iG} + x_{i}'\beta + \epsilon_{ij}.	\]
Note that if we include the spatial fixed effects, then we do not want to include the tract level variables as it would be superfluous. A common misconception in the literature is that spatial fixed effects "fix" the spatial autocorrelation. It only does for very specific cases. And those cases are pretty unlikely to hold. However, in some cases, after introducing the spatial fixed effects, there is no more evidence of residual spatial autocorrelation. So it has been "fixed". This is more of an empirical matter rather than a general rule. 

\subsubsection{Full Heterogeneity: Full Spatial Regimes}
This basically boils down to running a separate model in each of the subsets. The classical setup is 
\[	\begin{bmatrix}
	y_{1}\\
	y_{2}
\end{bmatrix} = \begin{bmatrix}
X_{1} & 0 \\
0 & X_{2}
\end{bmatrix} 
\begin{bmatrix}
	\beta_{1} \\
	\beta_{2}
\end{bmatrix} + 
\begin{bmatrix}
	\epsilon_{1}\\
	\epsilon_{2}
\end{bmatrix}.	\]
If there are no correlations across regimes, then it is better to write two regression equations instead.

\subsection{Testing for Spatial Heterogeneity}
 Our null hypothesis is equal intercepts and equal slopes.  The classical test on structural stability in econometrics is the Chow test \footnote{The Chow test is named after the Chinese-American economist Zhizhuang Zou.}. The basic idea is comparing two models - the restricted model and the unrestricted model. "Restricted" means you restrict some coefficients to be zero, and the "unrestricted" can be any number. Therefore, the restricted is always the simpler model. In our context, the "restricted" means all the coefficients are the same everywhere, and the "unrestricted" means flexibility for each of the subsets. 
 
 We could generalize the Chow test for a general test on coefficient variability. As an example, suppose we run two separate regressions for the regimes and get a full vector for the estimands $\beta' = (\beta_{11} \beta_{12} \beta_{13} \beta_{21} \beta_{22} \beta_{23})'$. We supply a matrix $R$, for example, specified as 
 \[	R  = \begin{bmatrix}
 	1 & 0 & 0 & -1 & 0 & 0 \\
 	0 & 1 & 0 & 0 & -1 & 0 \\
 	0 & 0 & 1 & 0 &  0 & -1
 \end{bmatrix}.	\]
Then our null hypothesis would be 
\[	H_{0}: R\beta = 0.	\]

This is a chi-square statistic ( a generalized Wald test), following the distribution 
\[	(R\hat{\beta})'[RVR']^{-1}(R\hat{\beta}) \sim \chi^{2}(G),	\]
where $V$ is the variance-covariance matrix of the $\hat{\beta}$. The tricky part lies on the identification of $V$. 

Note that this is perfectly flexible. You can test if $\beta_{12} = 2 \beta_{22}$ easily for example. Also, as long as you can come up with the $V$, no matter it comes from the spatial autoregressive model or the spatial lag error model.

\subsection{Spatial Regimes with Spatial Dependence}
We know how to deal with the regression coefficients part, but how should we deal with the spatial part? Do we keep the spatial process the same throughout? There is no clear direction except for simplicity. 

The spatial econometrics is based on asymptotics, and we need to think about the issue of whether the limit process stops at the border between regimes. Do not change the process unless you have a strong foundation. 

We have the two-regime spatial lag model with fixed spatial autoregressive coefficient
\[	\begin{bmatrix}
	y_{1}\\
	y_{2}
\end{bmatrix} = \rho W
\begin{bmatrix}
	y_{1}\\
	y_{2}
\end{bmatrix} + 
\begin{bmatrix}
	X_{1} & 0 \\
	0 & X_{2}
\end{bmatrix} 
\begin{bmatrix}
	\beta_{1}\\
	\beta_{2}
\end{bmatrix} + 
\begin{bmatrix}
	\epsilon_{1}\\
	\epsilon_{2}
\end{bmatrix}.	\]

The two-regime spatial lag model with varying spatial autoregressive coefficient is 
\[	\begin{bmatrix}
	y_{1}\\
	y_{2}
\end{bmatrix} = 
\begin{bmatrix}
	\rho_{1}W_{1} & 0 \\
	0 & \rho_{2}W_{2}
\end{bmatrix}
\begin{bmatrix}
	y_{1}\\
	y_{2}
\end{bmatrix} + 
\begin{bmatrix}
	X_{1} & 0 \\
	0 & X_{2}
\end{bmatrix} 
\begin{bmatrix}
	\beta_{1}\\
	\beta_{2}
\end{bmatrix} + 
\begin{bmatrix}
	\epsilon_{1}\\
	\epsilon_{2}
\end{bmatrix}.	\]
Note that there is no spillover between the two regimes. Similarly for the spatial error model.

\subsection{Spatially Varying Coefficients}\hfill\par 
\subsubsection{Expansion Method}
One way to deal with varying coefficients is by imposing structures on the coefficients. The initial model would be 
\[	y_{i} = \alpha + x_{i}\beta_{i} + \epsilon_{i}.	\]
Note the extreme heterogeneity here. This is an incidental parameter problem. We have the expansion equation
\[	\beta_{i} = \gamma_{0} + z_{i1}\gamma_{1} + z_{i2}\gamma_{2}.	\]
Some algebra shows that 
\[	y_{i} = \alpha + x_{i}\gamma_{0} + (z_{i1}x_{i})\gamma_{1} + (z_{i2}x_{i})\gamma_{2} + \epsilon_{i}.	\]
We ended up with a equation with interaction effects. We should be aware of the multicollinearity problem and the danger of overfitting. We can also let random error presents in the expansion equation
\[\beta_{i} = \gamma_{0} + z_{i1}\gamma_{1} + z_{i2}\gamma_{2} + \psi_{i}\]
and get the random expansion model.

\subsubsection{Geographically Weighted Regression}\hfill\par 
This is a special case of local regression. Local regression is about the attribute spaces, the space of variables $y$ and $x$s.  GWS is about the geographical space. Classical local regression exploits local subsets in the attribute dimensions, GWS exploits local subsets of the attribute, but in geographic space. The parameter values are obtained from a subset of observations using kernel regression. 

Suppose we are interested in the conditional expectation, i.e., the regression. For a simple bivariate regression, we have 
\[	y_{i} = m(x_{i}) + u_{i},	\]
where $m$ is an unspecified functional form. The kernel regression would be 
\[	m(x_{0}) = \sum_{i} K[(x_{i} - x_{0})/h] y_{i},	\]
where $K$ is our kernel function, and $h$ is the bandwidth. This is the classic kernel regression.

In the GWR setup, local estimation is based on nearby locations. But note that our local estimation is now based on the $x-y$ pairs at nearby locations. This give us a difficulty for interpretation - there is simply no asymptotics by adding more units in the nearby locations. Mathematically, we have 
\[	b(u_{i}, v_{i}) = [X'W(u_{i}, v_{i})X]^{-1}X'W(u_{i}, v_{i})y,	\]
where $W(u_{i}, v_{i})$ is a matrix with weights in the diagonal elements.

 

\printbibliography %Prints bibliography
		
\end{document}
